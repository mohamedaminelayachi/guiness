{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "kxk9RTLctU9P"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pz0vnNJvyg5b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### PRE-RELEASE CONFIG ########\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q-6h9wA0OI_",
        "outputId": "9c8732ad-1023-470c-ad30-a6f21e7a66e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3e4209b050>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################## DATASET LOADING #############################\n",
        "with open('sentences.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "5S_lf35-0HF6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################## TOKENIZATION ################################\n",
        "def encoder(text: str):\n",
        "  \"\"\"\n",
        "  the encoder() function takes text as an input and return back its tokenized\n",
        "  form.\n",
        "  :param text : the text you're willing to encode.\n",
        "  :return: the tokenized form of the text passed as a parameter.\n",
        "  \"\"\"\n",
        "  chars = sorted(list(set(text)))\n",
        "  words_to_int_map = { ch:i for i,ch in enumerate(chars) }\n",
        "  encoded = [words_to_int_map[c] for c in text]\n",
        "  chars_length = len(chars)\n",
        "  return encoded, chars_length\n",
        "\n",
        "def decoder(vector: str):\n",
        "  \"\"\"\n",
        "  the decoder() function takes encoded takes as an input and returns back its\n",
        "  original form.\n",
        "  :param vector : the text you're willing to encode.\n",
        "  :return: the original form of the text passed as a parameter.\n",
        "  \"\"\"\n",
        "  chars = sorted(list(set(text)))\n",
        "  int_to_words_map = {i:ch for i,ch in enumerate(chars)}\n",
        "  decoded = ''.join([int_to_words_map[i] for i in vector])\n",
        "  chars_length = len(chars)\n",
        "  return decoded, chars_length"
      ],
      "metadata": {
        "id": "uWdiDyg50iRi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################### DATASET SPLITS #################################\n",
        "encoded, vocab_size = encoder(text)\n",
        "######### DATASET = TRAINING SET + TESTING SET ##############\n",
        "data = torch.tensor(encoded, dtype=torch.long) # The encoded tensor\n",
        "tl = int(0.8*len(data)) \n",
        "train_data = data[:tl]\n",
        "test_data = data[tl:]"
      ],
      "metadata": {
        "id": "1bvCK0cM7Kjx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################## HYPERPARAMETERS #################################\n",
        "batch_size = 64\n",
        "block_size = 256 \n",
        "max_iters = 2000\n",
        "eval_interval = 200\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 400\n",
        "n_embd = 300\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "-aPq1J958C5V"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4acPgAwjr7Sb",
        "outputId": "76310a1b-8108-4d38-f63f-f54ac3f1e1ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.444859 Million Parameters\n",
            "Step 0: Train Loss 4.2254, Test Loss 4.2274\n",
            "Step 200: Train Loss 2.2631, Test Loss 2.4177\n",
            "Step 400: Train Loss 1.0520, Test Loss 2.1896\n",
            "Step 600: Train Loss 0.1559, Test Loss 3.2147\n",
            "Step 800: Train Loss 0.0724, Test Loss 4.0043\n",
            "Step 1000: Train Loss 0.0593, Test Loss 4.3786\n",
            "Step 1200: Train Loss 0.0519, Test Loss 4.6639\n",
            "Step 1400: Train Loss 0.0482, Test Loss 4.8161\n",
            "Step 1600: Train Loss 0.0457, Test Loss 4.8988\n",
            "Step 1800: Train Loss 0.0439, Test Loss 5.0649\n",
            "Step 1999: Train Loss 0.0429, Test Loss 5.1515\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def batchifier(split):\n",
        "    \"\"\"\n",
        "    batchifier is used to generate a small batch of inputs and targets.\n",
        "    :param split: the split from which you want to generate the batch.\n",
        "    :return: the inputs and targets.\n",
        "    \"\"\"\n",
        "    data = train_data if split == 'train' else test_data\n",
        "    random_set = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x_batch= torch.stack([data[i:i+block_size] for i in random_set])\n",
        "    y_batch = torch.stack([data[i+1:i+block_size+1] for i in random_set])\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "    return x_batch, y_batch\n",
        "\n",
        "@torch.no_grad()\n",
        "def loss_estimator():\n",
        "    \"\"\"\n",
        "    loss_estimator is used to estimate the loss of at every training step.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'test']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = batchifier(split)\n",
        "            tokenized_words, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" The head class represents the head of the attention mechanism in the \n",
        "    transformer.\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        batches,time,channels = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        weights = q @ k.transpose(-2,-1) * channels**-0.5\n",
        "        weights = weights.masked_fill(self.tril[:time, :time] == 0, float('-inf'))\n",
        "        weights = nn.functional.softmax(weights, dim=-1)\n",
        "        weights = self.dropout(weights)\n",
        "        v = self.value(x)\n",
        "        out = weights @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" A parallel attention mechanism that uses mulitple heads working in parallel\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        MultiHeadAttention.forward() is used to concatenate between different output\n",
        "        in a parallel context.\n",
        "        \"\"\"\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" The Feed Forward Neural Network is used reformulate the inputs in a \n",
        "    digestable form.\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.freeforward_net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.freeforward_net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.layer_norm_1 = nn.LayerNorm(n_embd)\n",
        "        self.layer_norm_2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.layer_norm_1(x))\n",
        "        x = x + self.ffwd(self.layer_norm_2(x))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        batches, time = idx.shape\n",
        "        token_embeddings = self.token_embedding_table(idx)\n",
        "        positional_embedding = self.position_embedding_table(torch.arange(time, device=device))\n",
        "        x = token_embeddings + positional_embedding\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            batches, time, channels = logits.shape\n",
        "            logits = logits.view(batches*time, channels)\n",
        "            targets = targets.view(batches*time)\n",
        "            loss = nn.functional.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate_tokens(self, wid, max_new_tokens):\n",
        "        \"\"\"\n",
        "        generate_tokens is used to generate new words based on the training that\n",
        "        has been doing.\n",
        "\n",
        "        :param wid: is an array that holds the indices in the current context\n",
        "        of size (B, T).\n",
        "        :param max_new_tokens: the maximum characters to generate.\n",
        "        \"\"\"\n",
        "      \n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop wid to the last block_size tokens\n",
        "            wid_cond = wid[:, -block_size:]\n",
        "            # Predict\n",
        "            tokenized_words, loss = self(wid_cond)\n",
        "            # Focus only on the last time step\n",
        "            tokenized_words = tokenized_words[:, -1, :]\n",
        "            # Apply softmax to get probabilities\n",
        "            probalistic_dist = nn.functional.softmax(tokenized_words, dim=-1)\n",
        "            # Sample from the distribution\n",
        "            wid_next = torch.multinomial(probalistic_dist, num_samples=1)\n",
        "            # Concatenate sampled index to the running sequence\n",
        "            wid = torch.cat((wid, wid), dim=1)\n",
        "        return wid\n",
        "    \n",
        "model = Transformer()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'Million Parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = loss_estimator()\n",
        "        print(f\"Step {iter}: Train Loss {losses['train']:.4f}, Test Loss {losses['test']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = batchifier('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    tokenized_words, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decoder(m.generate_tokens(context, max_new_tokens=2000)[0].tolist())) ######### Input 1 - MOHAMED CHOUKRI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hR8CcPAuAvR",
        "outputId": "08a902a3-7863-4330-d76b-4da2f313d188"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "is suddenly asleeping alone. Then I turned to Inher tree a pleasy that third I rubbed\n",
            "slept their as gener, sad attnames that Quess the matter back\n",
            "159\n",
            "\n",
            "\fMOHAMED C H O U K R I\n",
            "\n",
            "merrip in ordered, and I reached thinking the\n",
            "trutting stomalking towards throat, wonder object s bend overhead,\n",
            "how to the Comes dog.\n",
            "Come on. Get out of the watches. W hen I got a speak up it on\n",
            "like a litd car neither?\n",
            "No.\n",
            "19\n",
            "\n",
            "\fMOHAMED C H O UK R I\n",
            "\n",
            "The tire in bed, he climbed ove his it out into a\n",
            "room into the alread with all, with\n",
            "the rapart notice to Tetuan,\n",
            "and the police of the gatech. What dogs him? Choukri open the way country up\n",
            "the btablackets.\n",
            "And looks at the old door is the cows. I continued the bottl\n",
            "and sitting the darks with it.\n",
            "W hat a difficulwards the Rif stood two that lefty is not my little. One\n",
            "generminuto some so for see where it seven inte me, she saids even two\n",
            "or by the entrance was job in So norry. I thouse evet one\n",
            "of a busy years footsteps and asking a syuffered, until as I did.\n",
            "Here went with where you’re very send sno live you. I usual.\n",
            "You’re six herch.\n",
            "It s no more here across of more to her, she said.\n",
            "Why. You do ?\n",
            "He know Kebtaoui! Go a Comero if you\n",
            "worr talking to some in. You\n",
            "unlace here last as the stating, wife too. I know thateveral though I was.\n",
            "Lucking is it we continued Spartion.\n",
            "Then he was laughinatination we were, saadfasting welled aways,\n",
            "and orance.\n",
            "The girls ared all because on to steal. They of the pavemen are\n",
            "going to our poor to her hyses toot? I asked my my sex with her.\n",
            "She was signsy.\n",
            "She more anxicty. The could spide Barche.\n",
            "Then pushed to eah sound around.\n",
            "Watching, until\n",
            "on, and lightly with her, the other’s grabs soiling.\n",
            "I turned to sell out a heelicinal'roused to look at at Ain Drancs.\n",
            "\n",
            "I see eithe two written and three arried the peseta of balcony.\n",
            "When the gutt people sound of his hands of matches on the\n",
            "firsts. He drunk wis which comes back.\n",
            "I could run of the door and the window opene of so he is hand\n",
            "said awards very a woke\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decoder(m.generate_tokens(context, max_new_tokens=1000)[0].tolist())) ###### Generation 2 - Darija"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSs_3wrNfW08",
        "outputId": "add25e2e-0c47-48f6-dfcf-7b3218e49820"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "wach kent li af?\n",
            "kan nak f jami3 l7alat ljow f merrakch zine.\n",
            "kat3jbni chemch.\n",
            "bghit ghir ntbessi bl aSfo f ch7al had nkon rasna awla karta.\n",
            "hadchi bikhir o 3aSir dial limoun sghir?\n",
            "o wach bghiti telj fikra dial bghiti l9hwa fih?\n",
            "Caputshino?\n",
            "\"bghit chi kass kbir dial limoun m3Sor, bla telj.\"\n",
            "ayeh mer7at l program dial ter\n",
            "o kayban lia llah merrak li kaybo lia lina merra o tsowe\n",
            "jmhdma walo man kay3jboni lia lmossansa marsh la dial chi wa7d yji y3t9na\n",
            "\"ayeh, sir 3ellah, njerbo!\"\n",
            "Aaaaaaah\n",
            "\"katre33ed, yak chi bass makayn?\"\n",
            "bessa7?\n",
            "\"tana mstressi, frask wa7d tiyara ta7t lbar7 f Texas?\"\n",
            "idan 7na f nefs l7ala\n",
            "Tbib mabghi lia chwia dial ta bghiti.\n",
            "\"mangoulch l kolchi ghadi ykheli l balk melhi.\n",
            "achmen l3bat 3zaz 3lik?\n",
            "Poker!\n",
            "wla 3lach mat9tare7ch chi 7aja\n",
            "\"Ah, nl3b poker.\"\n",
            "hadi hia ll3ba piano? kantmenna\n",
            "kanl3b lpiano\n",
            "kandir piano merra merra o kayl3b lia lwa7d\n",
            "o men lkhdma wach kaykhaSS khrj m3a3ndich mak lsin tkhiyya bezzaf dial l3ilaj?\n",
            "rak kat7eTTini f wD3ia S3iba.\n",
            "ana rah Tbib nefsi dialk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decoder(m.generate_tokens(context, max_new_tokens=1000)[0].tolist())) ####### Generation 3 - Darija"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlKm7gMohNlz",
        "outputId": "79f9b649-dccb-4330-c878-f746b1302aca"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "mlli tan9der njib.\n",
            "o chi kass kbir had chi 9Ser nsber l dakchi.\n",
            "\"fekkenti kantSenneT lik o o men hadchi kachfek... anta 3ndich kandir had li kank.\n",
            "\"machi mochkil 3ndi la did bezzaf dial lkhdma, walakin lan.\"\n",
            "\"mazal 3ndi snani kamlin t9riban, hamdollah!\"\n",
            "bghitih tgherer!\n",
            "\"klghalib atgoul l alkarta diali kayn kanharj 7sen men dima 7aS kon hna fl3mer dialk.\n",
            "wach ma3ndi l3arD 3la Rachid.\n",
            "\"achi bia ghadi ykhal mo7al l3mer, hadi makan dima ana la makana 3zal 9bel jadawil Darb\"\n",
            "jadawil DDarb?\n",
            "\"ayeh, 3reft fchkel had l9adia\"\n",
            "walakin l ostada diali diali dik lw9t kant kadowwez lwa cha o teha hadial  kaykhiyyna chi sohel.\n",
            "hadi fikra momtana o lmada lli ghadi nha chab ghir fl9ent, o b9a kayjerr f ser ykh leb bezzaf dial l7lwa bach tkon bikhir.\n",
            "\"fekkertini, kantmenna matkonch daba, bach tbent lia howa tla bghiti.\"\n",
            "walakin chkon daba hadik Anya kantkheliwha bou7dha\n",
            "ila li wellaSSa la khar\n",
            "o 3ndi moun ghadi yjib lntibah md lkhrin.\n",
            "\"walakin chwia dial ttout bezzaf.\n",
            "\"hadchi kantmenna kanl3ik f jami3 l\n"
          ]
        }
      ]
    }
  ]
}